# docker-compose.yml
version: '3.8'

services:
  spark-master:
    image: my-spark:3.5
    container_name: spark-master
    ports:
      - "8080:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master Port
    volumes:
      - ./src/:/opt/bitnami/spark/src
      # Mount the centralized configuration file
      - ./conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - AWS_REGION=ap-south-1
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - MINIO_ENDPOINT=http://minio:9000

  spark-worker:
    image: my-spark:3.5
    container_name: spark-worker-1
    depends_on:
      - spark-master
    # --- ADD THESE LINES TO ALLOCATE MORE RESOURCES ---
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: '5g'
    volumes:
      - ./src/:/opt/bitnami/spark/src
#       Mount the same centralized configuration file
      - ./conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - AWS_REGION=ap-south-1
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - MINIO_ENDPOINT=http://minio:9000

  jupyterlab:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyterlab
    ports:
      - "8888:8888"
      - "4040:4040"
    volumes:
      - ./data/notebooks:/home/jovyan/work
      - ./src:/opt/bitnami/spark/src
      - ./conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    environment:
      # Set the Spark Master URL for this container to connect to
      - SPARK_MASTER_URL=spark://spark-master:7077
      # Add environment variables for accessing MinIO
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - MINIO_ENDPOINT=http://minio:9000
      - AWS_REGION=ap-south-1
    deploy:
      resources:
        limits:
          memory: 5g
    depends_on:
      - spark-master

  minio:
    image: minio/minio:latest
    container_name: minio_server
    ports:
      - "9000:9000"  # MinIO API Port
      - "9001:9001"  # MinIO Console Port
    volumes:
      - ./data/minio:/data
    environment:
      - MINIO_ROOT_USER=${AWS_ACCESS_KEY_ID}
      - MINIO_ROOT_PASSWORD=${AWS_SECRET_ACCESS_KEY}
    command: server /data --console-address ":9001"

  rest-catalog:
    image: tabulario/iceberg-rest
    container_name: rest-catalog
    ports:
      - "8181:8181"
    environment:
      - CATALOG_WAREHOUSE=s3a://raw/
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - CATALOG_S3_ENDPOINT=http://minio:9000
      # Use the CATALOG_ prefix for S3 properties, with double underscores for dots
      - CATALOG_S3_ACCESS__KEY__ID=${AWS_ACCESS_KEY_ID}
      - CATALOG_S3_SECRET__ACCESS__KEY=${AWS_SECRET_ACCESS_KEY}
      # Also add path-style access for the catalog's client
      - CATALOG_S3_PATH__STYLE__ACCESS=true
      # Add the region for the catalog's client
      - CATALOG_AWS__REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}