# ./conf/spark-defaults.conf

# spark.driver.extraClassPath /opt/bitnami/spark/jars/*
# spark.executor.extraClassPath /opt/bitnami/spark/jars/*


# -- 1. Spark Application Resources --
# Adjust these values based on your Docker Desktop memory/CPU allocation.
spark.driver.memory           2g
spark.executor.memory         2g
spark.executor.memoryOverhead 1g
spark.executor.cores          2
spark.sql.shuffle.partitions  100
# Adaptive Query Execution (AQE)
spark.sql.adaptive.enabled    true

# -- 2. Dependency Management (JAR Packages) --
# This is the complete list of all required JARs we discovered.
spark.jars.packages   org.apache.hadoop:hadoop-aws:3.3.4,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.iceberg:iceberg-aws-bundle:1.5.2
# spark.jars.packages   org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-client-runtime:3.3.4,org.apache.hadoop:hadoop-client-api:3.3.4,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2,org.apache.iceberg:iceberg-aws:1.9.2,software.amazon.awssdk:s3:2.25.25,software.amazon.awssdk:url-connection-client:2.25.25,software.amazon.awssdk:sts:2.25.25,software.amazon.awssdk:glue:2.25.25,software.amazon.awssdk:kms:2.25.25,software.amazon.awssdk:dynamodb:2.25.25
# spark.jars.packages   org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-client-runtime:3.3.4,org.apache.hadoop:hadoop-client-api:3.3.4,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2,org.apache.iceberg:iceberg-aws:1.9.2,org.apache.avro:avro:1.11.3,software.amazon.awssdk:s3:2.25.25,software.amazon.awssdk:url-connection-client:2.25.25,software.amazon.awssdk:sts:2.25.25,software.amazon.awssdk:glue:2.25.25,software.amazon.awssdk:kms:2.25.25,software.amazon.awssdk:dynamodb:2.25.25
# REMOVED the redundant hadoop-client-* JARs to resolve the ClassCastException
# spark.jars.packages   org.apache.hadoop:hadoop-aws:3.3.4,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2,org.apache.iceberg:iceberg-aws:1.9.2,org.apache.avro:avro:1.11.3,software.amazon.awssdk:s3:2.25.25,software.amazon.awssdk:url-connection-client:2.25.25,software.amazon.awssdk:sts:2.25.25,software.amazon.awssdk:glue:2.25.25,software.amazon.awssdk:kms:2.25.25,software.amazon.awssdk:dynamodb:2.25.25

# -- 3. Hadoop S3A Client Configuration (for general s3a:// access) --

spark.hadoop.fs.s3a.endpoint                      http://minio:9000
spark.hadoop.fs.s3a.access.key                    minioadmin
spark.hadoop.fs.s3a.secret.key                    minioadmin
spark.hadoop.fs.s3a.path.style.access             true
spark.hadoop.fs.s3a.connection.ssl.enabled        false
spark.hadoop.fs.s3a.impl                          org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.aws.credentials.provider      org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider

# Set the AWS Region for the Hadoop S3A client and other potential clients.
spark.hadoop.aws.region                           ap-south-1
spark.hadoop.fs.s3a.aws.region                    ap-south-1

# --- 4Iceberg REST Catalog Configuration ---
spark.sql.catalog.rest_catalog                            org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.rest_catalog.catalog-impl               org.apache.iceberg.rest.RESTCatalog
spark.sql.catalog.rest_catalog.uri                        http://rest-catalog:8181
spark.sql.catalog.rest_catalog.warehouse                  s3a://vod/

# -- 5. Iceberg S3FileIO Client Configuration --
# These properties are passed specifically to Iceberg's modern S3 client.
spark.sql.catalog.rest_catalog.io-impl                    org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.rest_catalog.s3.endpoint                http://minio:9000
spark.sql.catalog.rest_catalog.s3.path-style-access       true
spark.sql.catalog.rest_catalog.s3.access-key-id           minioadmin
spark.sql.catalog.rest_catalog.s3.secret-access-key       minioadmin

# Set a custom path for the Ivy cache
spark.jars.ivy           /tmp/.ivy2